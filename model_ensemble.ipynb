{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Ensemble: This code performs model ensemble for the top-3 performing models from Model-C using different ensemble strategies such as simple averaging, weighted averaging, majority voting, and stacking. For the stacking ensemble, a neural-network based meta-learner is used to learn to combine the predictions of the top-3 performing models from model-C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, SeparableConv2D, BatchNormalization, ZeroPadding2D, GlobalAveragePooling2D,Flatten,Average, BatchNormalization, Dropout\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import applications\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important note: The size of the batch size plays an important role. \n",
    "#check whether the number of train, validation and test samples are\n",
    "#absolutely divisible by the batch size. if not, make sure to add 1\n",
    "# (+1) while fitting, evaluting and testing like this: do not use workers=1\n",
    "#reset the generators everytime before using them otherwise you will\n",
    "#get wierd results\n",
    "\n",
    "img_width, img_height = 224,224\n",
    "train_data_dir = 'cv5/train'\n",
    "test_data_dir = 'cv5/test'\n",
    "epochs = 64\n",
    "batch_size = 8 #check if absolutely divisible for train, validation and test, else follow the procedure discussed above\n",
    "num_classes= 2\n",
    "\n",
    "# Since the models work with the data of the same shape, we \n",
    "#define a single input layer that will be used by every model.\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom definition for confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False, #if true all values in confusion matrix is between 0 and 1\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare data generators\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=2,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=0.5,\n",
    "      horizontal_flip=False,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% assign class weights to balance model training and penalize over-represented classes\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the custom model\n",
    "\n",
    "def custom_cnn(model_input):\n",
    "    x = BatchNormalization()(model_input)\n",
    "    x = Conv2D(64, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=model_input, outputs=x, name='custom_cnn')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "custom_model = custom_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(custom_model, to_file='custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained custom model on large-scale CXR collection\n",
    "\n",
    "custom_model.load_weights('custom_cnn.45-0.8608.h5')\n",
    "\n",
    "#print model summary\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% VGG16 model \n",
    "\n",
    "def vgg16_cnn(model_input):\n",
    "    vgg16_cnn = VGG16(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    vgg16_cnn = Model(inputs=vgg16_cnn.input, outputs=vgg16_cnn.get_layer('block5_conv3').output)\n",
    "    x = vgg16_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=vgg16_cnn.input, outputs=predictions, name='vgg16_custom')\n",
    "    model.get_layer(name='block1_conv1').name='block1_conv1VGG'  \n",
    "    model.get_layer(name='block1_conv2').name='block1_conv2VGG' \n",
    "    model.get_layer(name='block2_conv1').name='block2_conv1VGG' \n",
    "    model.get_layer(name='block2_conv2').name='block2_conv2VGG' \n",
    "    model.get_layer(name='block3_conv1').name='block3_conv1VGG' \n",
    "    model.get_layer(name='block3_conv2').name='block3_conv2VGG' \n",
    "    model.get_layer(name='block3_conv3').name='block3_conv3VGG' \n",
    "    model.get_layer(name='block4_conv1').name='block4_conv1VGG' \n",
    "    model.get_layer(name='block4_conv2').name='block4_conv2VGG' \n",
    "    model.get_layer(name='block4_conv3').name='block4_conv3VGG' \n",
    "    model.get_layer(name='block5_conv1').name='block5_conv1VGG' \n",
    "    model.get_layer(name='block5_conv2').name='block5_conv2VGG' \n",
    "    model.get_layer(name='block5_conv3').name='block5_conv3VGG' \n",
    "    model.get_layer(name='block1_pool').name='block1_poolVGG' \n",
    "    model.get_layer(name='block2_pool').name='block2_poolVGG' \n",
    "    model.get_layer(name='block3_pool').name='block3_poolVGG' \n",
    "    model.get_layer(name='block4_pool').name='block4_poolVGG' \n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "vgg16_custom_model = vgg16_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(vgg16_custom_model, to_file='vgg16_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CXR trained VGG16 model pretrained on large-scale CXR collection\n",
    "\n",
    "vgg16_custom_model.load_weights('vgg16_custom.13-0.8955.h5')\n",
    "\n",
    "#print model summary\n",
    "vgg16_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Inception-V3 model \n",
    "\n",
    "def inceptionv3_cnn(model_input):\n",
    "    inceptionv3_cnn = InceptionV3(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = inceptionv3_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inceptionv3_cnn.input, outputs=predictions, name='InceptionV3_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "inceptionv3_custom_model = inceptionv3_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "inceptionv3_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(inceptionv3_custom_model, to_file='inceptionv3_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CXR trained InceptionV3 model \n",
    "\n",
    "inceptionv3_custom_model.load_weights('InceptionV3_custom.17-0.8957.h5')\n",
    "\n",
    "#print model summary\n",
    "inceptionv3_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% InceptionResNetV2 model \n",
    "\n",
    "def incepres_cnn(model_input):\n",
    "    incepres_cnn = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = incepres_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=incepres_cnn.input, outputs=predictions, name='InceptionResnet_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "inceptionresnet_custom_model = incepres_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "inceptionresnet_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(inceptionresnet_custom_model, to_file='inceptionresnet_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CXR trained InceptionResNet-V2 model\n",
    "\n",
    "inceptionresnet_custom_model.load_weights('InceptionResnet_custom.24-0.8960.h5')\n",
    "\n",
    "#print model summary\n",
    "inceptionresnet_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Xception model \n",
    "\n",
    "def xception_cnn(model_input):\n",
    "    xception_cnn = Xception(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = xception_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=xception_cnn.input, outputs=predictions, name='xception_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "xception_custom_model = xception_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "xception_custom_model.summary()\n",
    "\n",
    "#plot model summary\n",
    "plot_model(xception_custom_model, to_file='xception_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CXR trained Xception model\n",
    "\n",
    "xception_custom_model.load_weights('xception_custom.06-0.8870.h5')\n",
    "\n",
    "#print model summary\n",
    "xception_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DenseNet121 model\n",
    "\n",
    "def densenet_cnn(model_input):\n",
    "    densenet_cnn = DenseNet121(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = densenet_cnn.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=densenet_cnn.input, outputs=predictions, name='densenet121_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "densenet_custom_model = densenet_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "densenet_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(densenet_custom_model, to_file='densenet_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the CXR trained DenseNet121 model\n",
    "\n",
    "densenet_custom_model.load_weights('densenet121_custom.18-0.8966.h5')\n",
    "\n",
    "#print model summary\n",
    "densenet_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compile and train the pretrained custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "filepath = 'weights/' + custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% compile and train the VGG16 model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg16_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + vgg16_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = vgg16_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% compile and train the InceptionV3 model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "inceptionv3_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "filepath = 'weights/' + inceptionv3_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = inceptionv3_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"InceptionV3_custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train Incetpion ResNet-V2\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "inceptionresnet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "filepath = 'weights/' + inceptionresnet_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = inceptionresnet_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=32, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"inceptionresnet_custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% compile and train the Xception model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "xception_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + xception_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = xception_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=32, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Xception_custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% compile and train the DenseNet121 model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "densenet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + densenet_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = densenet_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=32, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot and save performance graphs\n",
    "\n",
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"DenseNet_custom_plot_modelc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "#custom model\n",
    "\n",
    "custom_model.load_weights('weights/custom_cnn.02-0.8561.h5')\n",
    "custom_model.summary()\n",
    "\n",
    "#compile the custom model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy\n",
    "scorecustom = custom_model.evaluate_generator(validation_generator, nb_validation_samples // batch_size + 1, verbose = 1)\n",
    "print(\"Validation Accuracy = \",scorecustom[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance on test data\n",
    "\n",
    "#first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "custom_y_pred = custom_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "\n",
    "#print prediction shapes\n",
    "print(custom_y_pred.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure performance metrics\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VGG-16\n",
    "\n",
    "vgg16_custom_model.load_weights('weights/vgg16_custom.16-0.8731.h5')\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "vgg16_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy, make sure the batch size is absolutely divisible or add +1\n",
    "scorevgg16 = vgg16_custom_model.evaluate_generator(validation_generator, nb_validation_samples // batch_size + 1, verbose=1)\n",
    "print(\"Validation Accuracy = \",scorevgg16[1])\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg16_custom_y_pred = vgg16_custom_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "\n",
    "#print prediction shapes\n",
    "print(vgg16_custom_y_pred.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance metrics\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,vgg16_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "from itertools import cycle\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        vgg16_custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], vgg16_custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   vgg16_custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, vgg16_custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,vgg16_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Xception model by loading the best weights\n",
    "\n",
    "xception_custom_model.load_weights('weights/xception_custom.09-0.8507.h5')\n",
    "xception_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "xception_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy, make sure the batch size is absolutely divisible or add +1\n",
    "scorexception = xception_custom_model.evaluate_generator(validation_generator, nb_validation_samples // batch_size + 1, verbose=1)\n",
    "print(\"Validation Accuracy = \",scorexception[1])\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy\n",
    "xception_custom_y_pred = xception_custom_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "print(xception_custom_y_pred.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance metrics\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,xception_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        xception_custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], xception_custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   xception_custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, xception_custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,xception_custom_y_pred,figsize=(20,10),\n",
    "                                title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the DenseNet model by loading the best weights\n",
    "\n",
    "densenet_custom_model.load_weights('weights/densenet121_custom.15-0.8657.h5')\n",
    "densenet_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "densenet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy, make sure the batch size is absolutely divisible or add +1\n",
    "scoredensenet = densenet_custom_model.evaluate_generator(validation_generator, nb_validation_samples // batch_size + 1, verbose=1)\n",
    "print(\"Validation Accuracy = \",scoredensenet[1])\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy\n",
    "densenet_custom_y_pred = densenet_custom_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "print(densenet_custom_y_pred.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the performance metrics of the DenseNet model\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,densenet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        densenet_custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], densenet_custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   densenet_custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, densenet_custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,densenet_custom_y_pred,figsize=(20,10),\n",
    "                                title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Inception-V3 model by loading the best weights\n",
    "\n",
    "inceptionv3_custom_model.load_weights('weights/InceptionV3_custom.13-0.8731.h5')\n",
    "inceptionv3_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "inceptionv3_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy, make sure the batch size is absolutely divisible or add +1\n",
    "scoreinceptionv3 = inceptionv3_custom_model.evaluate_generator(validation_generator, \n",
    "                                                               nb_validation_samples // batch_size + 1, verbose=1)\n",
    "print(\"Validation Accuracy = \",scoreinceptionv3[1])\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy\n",
    "inceptionv3_custom_y_pred = inceptionv3_custom_model.predict_generator(test_generator, \n",
    "                                                                       nb_test_samples//batch_size + 1, verbose=1)\n",
    "print(inceptionv3_custom_y_pred.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the performance metrics of the Inception-V3 model\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,inceptionv3_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        inceptionv3_custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], inceptionv3_custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   inceptionv3_custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, inceptionv3_custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,inceptionv3_custom_y_pred,figsize=(20,10),\n",
    "                                title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the InceptionResnet model by loading the best weights\n",
    "\n",
    "inceptionresnet_custom_model.load_weights('weights/InceptionResnet_custom.10-0.8582.h5')\n",
    "inceptionresnet_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "inceptionresnet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "#reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "#calculate validation accuracy, make sure the batch size is absolutely divisible or add +1\n",
    "scoreinceptionresnet = inceptionresnet_custom_model.evaluate_generator(validation_generator,\n",
    "                                                                       nb_validation_samples // batch_size + 1, verbose=1)\n",
    "print(\"Validation Accuracy = \",scoreinceptionresnet[1])\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy\n",
    "inceptionresnet_custom_y_pred = inceptionresnet_custom_model.predict_generator(test_generator,\n",
    "                                                                               nb_test_samples//batch_size + 1, verbose=1)\n",
    "print(inceptionresnet_custom_y_pred.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the performance metrics of the InceptionResnet model\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),inceptionresnet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,inceptionresnet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        inceptionresnet_custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], inceptionresnet_custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   inceptionresnet_custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, inceptionresnet_custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,inceptionresnet_custom_y_pred,figsize=(20,10),\n",
    "                                title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing these evaluations, the Inception-V3, INceptionresNet-V2, and DenseNet-121 models from model-C were found to be the top-3 performing models. They are used to create ensemble models. Lets begin with Majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do a dummy assignment of the predictions\n",
    "\n",
    "inceptionv3_custom_y_pred1 = inceptionv3_custom_y_pred\n",
    "densenet_custom_y_pred1 = densenet_custom_y_pred\n",
    "inceptionresnet_custom_y_pred1 = inceptionresnet_custom_y_pred\n",
    "print(\"The shape of inceptionv3 custom model prediction inceptionv3_custom_y_pred is = \", inceptionv3_custom_y_pred1.shape)\n",
    "print(\"The shape of densenet custom model prediction densenet_custom_y_pred is  = \", densenet_custom_y_pred1.shape)\n",
    "print(\"The shape of Inception ResNet custom model prediction inceptionresnet_custom_y_pred is  = \", inceptionresnet_custom_y_pred.shape)\n",
    "\n",
    "#%% Max voting or majority voting\n",
    "inceptionv3_custom_y_pred1 = inceptionv3_custom_y_pred1.argmax(axis=-1)\n",
    "print(inceptionv3_custom_y_pred1)\n",
    "densenet_custom_y_pred1 = densenet_custom_y_pred1.argmax(axis=-1)\n",
    "print(densenet_custom_y_pred1)\n",
    "inceptionresnet_custom_y_pred1 = inceptionresnet_custom_y_pred1.argmax(axis=-1)\n",
    "print(inceptionresnet_custom_y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max voting begins: \n",
    "\n",
    "results = np.concatenate((inceptionresnet_custom_y_pred1.reshape(-1,1),\n",
    "                          inceptionv3_custom_y_pred1.reshape(-1,1),\n",
    "                          densenet_custom_y_pred1.reshape(-1,1)),\n",
    "                          axis=1)\n",
    "\n",
    "\n",
    "#Now perform the mode on that matrix across the single rows, \n",
    "#but all in one single operation (no need for a loop):\n",
    "max_voting = mode(results, axis=1)\n",
    "\n",
    "# The results contain two things: the mode values for each row \n",
    "#and the counts of that mode within that row.\n",
    "# To get the mode values, take the first element from max_voting_pred:\n",
    "max_voting_pred = max_voting[0]\n",
    "\n",
    "#calcualte majority voting accuracy\n",
    "ensemble_model_max_voting_accuracy = accuracy_score(Y_test,max_voting_pred)\n",
    "print(\"The max voting accuracy of the ensemble model is  = \", ensemble_model_max_voting_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,max_voting_pred,target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,max_voting_pred)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Max Voting ensemble without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predictions\n",
    "np.savetxt('max_voting_y_pred.csv',max_voting_pred,fmt='%i',delimiter = \",\")\n",
    "\n",
    "#evaluate error\n",
    "ensemble_model_maxvoting_mean_squared_error = mean_squared_error(Y_test,max_voting_pred)  \n",
    "ensemble_model_maxvoting_mean_squared_log_error = mean_squared_log_error(Y_test,max_voting_pred)  \n",
    "print(\"The max voting mean squared error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_error)\n",
    "print(\"The max voting mean squared log error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets perform simple averaging of the predictions from individual models\n",
    "\n",
    "average_pred=(inceptionresnet_custom_y_pred + inceptionv3_custom_y_pred +\n",
    "              densenet_custom_y_pred)/3\n",
    "\n",
    "#compute simple averaging accuracy\n",
    "ensemble_model_averaging_accuracy = accuracy_score(Y_test,average_pred.argmax(axis=-1))\n",
    "print(\"The averaging accuracy of the ensemble model is  = \", ensemble_model_averaging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Average Ensemble without normalization')\n",
    "\n",
    "plt.show()\n",
    "#save the predictions\n",
    "np.savetxt('averagring_y_pred.csv',average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curves\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,average_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        average_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], average_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   average_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, average_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate simple averaging error\n",
    "\n",
    "ensemble_model_averaging_mean_squared_error = mean_squared_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "ensemble_model_averaging_mean_squared_log_error = mean_squared_log_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "print(\"The averaging mean squared error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_error)\n",
    "print(\"The averaging mean squared log error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted averaging: weighing the predictions based on the performance of the models. This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. With DenseNet and IRV2 performing equally better, we award equal importance to them and higher importance to InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted averaging\n",
    "weighted_average_pred=(inceptionresnet_custom_y_pred * 0.25 +\n",
    "                       inceptionv3_custom_y_pred * 0.5 +\n",
    "                       densenet_custom_y_pred * 0.25)\n",
    "\n",
    "#calculate weighted averaging accuracy\n",
    "ensemble_model_weighted_averaging_accuracy = accuracy_score(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "print(\"The weighted averaging accuracy of the ensemble model is  = \", ensemble_model_weighted_averaging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,weighted_average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Weighted Average Ensemble without normalization')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weighted_averaging_y_pred.csv',weighted_average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curves\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,weighted_average_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        weighted_average_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], weighted_average_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   weighted_average_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, weighted_average_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "ensemble_model_weighted_average_mean_squared_error = mean_squared_error(Y_test,weighted_average_pred.argmax(axis=-1))  \n",
    "ensemble_model_weighted_average_mean_squared_log_error = mean_squared_log_error(Y_test,weighted_average_pred.argmax(axis=-1))  \n",
    "print(\"The weighted averaging mean squared error of the ensemble model is  = \", ensemble_model_weighted_average_mean_squared_error)\n",
    "print(\"The weighted averaging mean squared log error of the ensemble model is  = \", ensemble_model_weighted_average_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking: we attempted performing a stacking ensemble by training a meta-learner that will best combine the predictions from the sub-models and ideally perform better than any single sub-model.The first step is to load the saved models. We can use the load_model() Keras function and create a Python list of loaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "n_models = 3 #we have three models\n",
    "\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    inceptionresnet_custom_model.load_weights('weights/InceptionResnet_custom.09-0.8955.h5')\n",
    "    all_models.append(inceptionresnet_custom_model)\n",
    "    densenet_custom_model.load_weights('weights/densenet121_custom.06-0.9179.h5')\n",
    "    all_models.append(densenet_custom_model)\n",
    "    inceptionv3_custom_model.load_weights('weights/InceptionV3_custom.11-0.9328.h5')\n",
    "    all_models.append(inceptionv3_custom_model)\n",
    "    return all_models\n",
    "\n",
    "# We can call this function to load our three saved models from the “models/” sub-directory.\n",
    "# load all models\n",
    "\n",
    "n_members = 3\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to know how well the single models perform on the validation dataset and test dataset as we would expect a stacking model to perform better. We can easily evaluate each single model on the training dataset and establish a baseline of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate standalone models on the validation dataset\n",
    "for model in members:\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "    _, acc = model.evaluate_generator(validation_generator,nb_validation_samples // batch_size + 1, verbose=1)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) \n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    _, acc = model.evaluate_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "    print('Model Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Stacking Model: It may be desirable to use a neural network as a meta-learner.Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model. The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable. This can be achieved using the Keras functional interface for developing models. \n",
    "\n",
    "After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. All of the layers in each of the loaded models be marked as not trainable so the weights cannot be updated when the new larger model is being trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong. Once the sub-models have been prepared, we can define the stacking ensemble model. The input layer for each of the sub-models will be used as a separate input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, 3. The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 6-element vector will be created from the two class-probabilities predicted by each of the 3 models. \n",
    "\n",
    "We will then define a hidden layer to interpret this “input” to the meta-learner and an output layer that will make its own probabilistic prediction. A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "        # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(6, activation='relu')(merge) \n",
    "    drop1 = Dropout(0.1)(hidden) \n",
    "    output = Dense(2, activation='softmax')(drop1)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output, name = 'stacking_ensemble')\n",
    "    # compile\n",
    "    #sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "stacked_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(stacked_model, to_file='stacked_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined, it can be fit. We can fit it directly on the holdout validation dataset. Because the sub-models are not trainable, their weights will not be updated during training and only the weights of the new hidden and output layer will be updated. The stacking neural network model will be fit on the trainig data for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the ensemble model\n",
    "history = stacked_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                  epochs=300, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  #callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance of the ensemble model\n",
    "\n",
    "N = 300\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"stacking_ensemble_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once fit, we can use the new stacked model to make a prediction on new data.\n",
    "# This is as simple as calling the predict_generator() function on the model. \n",
    "\n",
    "#first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "ensemble_y_pred = stacked_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "\n",
    "#print prediction shapes\n",
    "print(ensemble_y_pred.shape)\n",
    "\n",
    "#ground truth label\n",
    "print(Y_test.shape)\n",
    "\n",
    "#measure performance metrics of the stacked ensemble\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,ensemble_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        ensemble_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], ensemble_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   ensemble_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, ensemble_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,ensemble_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model has its own weaknesses. The reasoning behind using an ensemble is that by stacking different models representing different hypotheses about the data, we can find a better hypothesis that is not in the hypothesis space of the models from which the ensemble is built. By using a very basic ensemble, a much lower error rate was achieved than when a single model was used. This proves effectiveness of ensembling. Of course, there are some practical considerations to keep in mind when using an ensemble for your machine learning task. Since ensembling means stacking multiple models together, it also means that the input data needs to be forward-propagated for each model. This increases the amount of \n",
    "#compute that needs to be performed and, consequently, evaluation (predicition) time. Increased evaluation time is not critical if you use an ensemble in research or in a Kaggle competition. However, it is a very critical factor when designing a commercial product. Another consideration is increased size of the final model which, again, might be a limiting factor for ensemble use in a commercial product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
